import scraper,crawl
from scraper import *
from crawl import *
import json
import cohere
from dotenv import load_dotenv, dotenv_values 
load_dotenv()
import os

class Rufus(Crawler,Scraper):
    def __init__(self,model = 'command-r'):
        '''
        Arguments : 
        model : string, takes the model name, can be any text generation model from Cohere.
        Notes
        This class is an interface for different methods to scrape the web.
        Todo : add api client code here
        '''
        # Authenticate and initialize Cohere client
        load_dotenv()
        self.co = cohere.Client(api_key=os.environ['COHERE_API_KEY'])
        
        self.model = model

        ## To do, authenticate and provide own API ?
        
        self.id = 'xyz'
    
    def scrape_url(self,url, handle_exception = True):
        '''
        Description :
        Wrapper for the Web scraper. Scrapes a single URL and contains methods for extracting human readable text as well as all outlinks.
        
        Arguments : 
        url : 'string' provide the url to be scraped
        handle_exception : True(default), if False returns a set Word, instead of exception
        '''
        self.scraper = Scraper(url,handle_exception)
        context_from_url = self.scraper.get_visible_text()
        res = self.clean_crawled_data(context_from_url)
        return {"cleaned_data": res , "raw_data": context_from_url}
    
    def deep_scrape_url(self,url,query = None,relavance_method = 'dpr', max_links = 10, topk = 5, depth = 3):
        '''
        Description : 
        Wrapper for the deep_scrape method. Recursively scrapes web outlinks. 
        Handles duplicates, if query is provided only topk contextual pages are scraped
        
        Arguments :
        url : root url to start exploration from
        query : default None, if provided will use relavance methods for collecting data from links, leave as None to extract every outlink and create a data store
        relavance_method : dpr or bm25, only used if query is not None
        depth : exploration depth starting from root node
        max_links: maximum number of urls to explore, if you donot want this to be a limiting factor, set to very high number but it will take a lot of time
        topk : used if query is present, selects the topk relavant answers.
        
        Returns : Answer from the deep_scraped context if query is provided, otherwise, 
                    scraping is performed and nothing is returned, you can call the API to save it to disk
        '''
        self.deep_scraper = deep_scrape(url = url,query = query,
                                        relavance_method = relavance_method,depth = depth,
                                        max_links = max_links,topk = topk)
        self.deep_scraper.recursive_scrape()
        
        if query:
            context = " "
            for k,v in self.deep_scraper.data_store.items():
                context += v + ' '
            return self.generate_answer(query,context,True,False)
        else:
            print("deep_scraping of the URL Done")
            print(" You can access the dictionary with <rufus_obj>.deep_scrape_url.deep_scraper.data_store, it is a dictionary")
            print(" You can save it to disk with save_data_to_file in deep_scraper")
        #return context
    
    def crawl_web(self,query,num_queries = 5):
        '''
        Description : 
        Wrapper for web crawler that uses google search to crawl the web and get back with the best matching URLs
        
        Arguments : 
        query : string, enter the text for your serach to begin
        num_queries : No. of search results to return 
        
        Returns :
        Answer generated by combining google search with LLM
        '''
        self.crawler = Crawler(query,num_queries=num_queries)
        context = self.crawler.get_data()
        return self.generate_answer(query,context,True,False)
        
    def clean_crawled_data(self,text):
        response = self.co.chat(
                    model=self.model,
                    message = text + '''Above is the text obtained from web scraping. 
                    Clean the above data so that it can be viewed by humans or could act as context for a Large language model.'''
                    )
        return response.text
    
    def generate_answer(self,query,context,print_ = True,clean_data=False):
        '''
        Description : 
        Given a query and contextual information, answers the question with respect to the context
        
        query : string, provide the question
        context : string, provide the context, from which the query can be answered
        print_ : bool, if True prints the answer before returning
        clean_data : (currently does nothing, can be used to clean the data before processing, might not be necessary though)
        '''
        response = self.co.chat(
            model=self.model,
            message = context + '''Above is the context obtained by scraping multiple webpages, process it appropriately and answer the query. Query = ''' + query
            )
        if print_:
            print(response.text)
        return response.text
        